= Who Needs Monitoring
include::../partials/versions.adoc[]
include::_attributes.adoc[]

Who needs monitoring, right?

image:this-is-fine.jpg[alt]

Well in case you think it's a good idea to monitor your application then keep reading and doing!

[#the-monitoring-stack]
== The Monitoring Stack

In OpenShift with regards to monitoring we use link:https://prometheus.io[Prometheus] as the more prominent piece of software, link:https://thanos.io[Thanos] is the other piece. In OpenShift monitoring is a key subject, you can read more about it link:https://docs.openshift.com/container-platform/{oc-version}/monitoring/monitoring-overview.html[here].

For this lab, let's concentrate on Prometheus. Prometheus is installed and maintained by an operator, the installation of an OpenShift cluster already takes care of installing Prometheus and maintaining it using the operator.

In case you don't know what an operator is please read link:https://www.redhat.com/en/technologies/cloud-computing/openshift/what-are-openshift-operators[this]. But for the sake of simplicity for now consider an operator as specialized logic run as a pod in a kubernetes cluster that takes care of installing and maintaining a system based on yaml definitions called link:https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources[CRD].

Let's get up to speed with regards to Prometheus. Prometheus is an open-source systems monitoring and alerting toolkit collects and stores its metrics as time series data, i.e. metrics information is stored with the timestamp at which it was recorded, alongside optional key-value pairs called labels.

Please note that Prometheus works with the typical behavior you've probably seen in real life... Don't call us we'll call you. Well Prometheus will call your workloads and expects metrics from it in a specific format. This means in our case where we have an Operator taking care of Prometheus that you have to crete an object of type `ServiceMonitor` that on one hand specifies a selector to find the kubernetes `Services` and on the other hand specifies the uri to call to get the metrics from.

Please, find bellow an example:

[source,yaml,attributes]
----
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    k8s-app: prometheus
  name: monitor
spec:
  endpoints:
    - interval: 30s
      path: /actuator/prometheus <1>
      port: http
  namespaceSelector: <2>
    any: true
  selector: <3>
    matchLabels:
      monitored: prometheus
----
<1> Uri to call and how often
<2> Namespaces to look for the Service compliant with our search
<3> Selector to find the Services objects that expose metrics in the given uri/path

[#an-spurious-error]
== An Spurious Error

Maybe in the code of your app there are errors that manifest only under certain circumstances... or randomly. Inadvertently you have injected an error somewhere. Let's execute a simple test, please run this command:

[.console-input]
[source,sh, subs="+macros,+attributes"]
----
for i in {1..20}; do curl -sI https://$(oc get route/fruit-gateway -o jsonpath='{.spec.host}')/api/config | grep HTTP; done
----

You should get an output similar to this one:

[.console-output]
[source,sh, subs="+macros,+attributes"]
----
HTTP/1.1 200 OK
HTTP/1.1 200 OK
HTTP/1.1 500 Internal Server Error
...
HTTP/1.1 200 OK
HTTP/1.1 200 OK
HTTP/1.1 500 Internal Server Error
----

In general you want find an error of this kind this way. Instead, you need a way to define a base line and detect abnormal situations, deviation from that baseline, etc.

One way to generate this baseline is defining custom logic/business related metrics. This means we have to do two things:

- Generate prometheus compliant metrics using a library preferably from our code 
- Configure Prometheus using a ServiceMonitor so that it can scrape the metrics we're generating.

Well, let's start by modifying the code.

[#generating-metrics]
== Generating Metrics

You have to generate

[source,yaml,attributes]
----
<dependency>
  <groupId>io.quarkus</groupId>
  <artifactId>quarkus-micrometer-registry-prometheus</artifactId>
</dependency>

----



But where is this error happening? How can I find out in an OpenShift cluster?


